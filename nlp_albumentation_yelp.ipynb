{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Albumentation with YELP Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\echemochek\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.cluster import KMeansClusterer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/yelp.csv\")\n",
    "dataAug = pd.read_csv(\"data/yelp_augmented.csv\")\n",
    "dataSentiment = pd.read_csv(\"data/yelpSentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSentiment = dataSentiment[[\"text\", \"polarity\"]]\n",
    "dataAug = dataAug[[\"augmented\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].str.lower()\n",
    "dataAug[\"augmented\"] = dataAug[\"augmented\"].str.lower()\n",
    "dataSentiment[\"text\"] = dataSentiment[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of each review\n",
    "data['text'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in each review\n",
    "data['text'].str.split().map(lambda x: len(x)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word length in each review\n",
    "data['text'].str.split().apply(lambda x : [len(i) for i in x]). map(lambda x: np.mean(x)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "corpus=[]\n",
    "txt = data['text'].str.split()\n",
    "txt = txt.values.tolist()\n",
    "corpus=[word for i in txt for word in i]\n",
    "\n",
    "from collections import defaultdict\n",
    "dic = defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize most frequent stopwords\n",
    "dic = dict(sorted(dic.items(), key=lambda item: item[1], reverse = True))\n",
    "dic_subset = {key:value for key, value in dic.items() if value > 4000}\n",
    "\n",
    "names = list(dic_subset.keys())\n",
    "values = list(dic_subset.values())\n",
    "\n",
    "plt.figure(figsize=(10,6)) \n",
    "plt.bar(range(len(dic_subset)), values, tick_label = names)\n",
    "plt.title(\"Most frequent stopwords\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent non-stopwords\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(corpus)\n",
    "most = counter.most_common()\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in most[:40]:\n",
    "    if (word not in stop):\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x).set(title='Most frequent non-stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent n-grams\n",
    "\n",
    "def get_top_ngram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) \n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams and trigrams plot\n",
    "\n",
    "def plotNgrams(df, colname, gram='bi'):\n",
    "    import seaborn as sns\n",
    "\n",
    "    # bigrams\n",
    "    if gram=='bi':\n",
    "        top_bi_grams = get_top_ngram(df[colname], n=2)\n",
    "        x_bi,y_bi = map(list,zip(*top_bi_grams))\n",
    "        plot = sns.barplot(x=y_bi, y=x_bi).set(title='Most frequent bigrams')\n",
    "\n",
    "    # trigrams\n",
    "    elif gram=='tri':\n",
    "        top_tri_grams = get_top_ngram(data['text'], n=3)\n",
    "        x_tri,y_tri = map(list,zip(*top_tri_grams))\n",
    "        plot = sns.barplot(x=y_tri, y=x_tri).set(title='Most frequent trigrams')\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNgrams(df=data, colname='text', gram='tri')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    corpus=[]\n",
    "    lem = WordNetLemmatizer()\n",
    "    for txt in data['text']:\n",
    "        words=[w for w in word_tokenize(txt) if (w not in stop)]\n",
    "        \n",
    "        words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "        \n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW model\n",
    "dic_bow = gensim.corpora.Dictionary(corpus)\n",
    "bow_corpus = [dic_bow.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 4, \n",
    "                                   id2word = dic_bow,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interacting with LDA output\n",
    "\n",
    "#!pip install pyldavis\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = gensimvis.prepare(lda_model, bow_corpus, dic_bow)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=100,\n",
    "        max_font_size=30,\n",
    "        scale=3,\n",
    "        random_state=42)\n",
    "   \n",
    "    wordcloud = wordcloud.generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# polarity\n",
    "def polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "# sentiment classification\n",
    "def sentiment(x):\n",
    "    if x< -0.2:\n",
    "        return 'neg'\n",
    "    elif x>=-0.2 and x<=0.2:\n",
    "        return 'neu'\n",
    "    else:\n",
    "        return 'pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, 'Distribution of Polarity Score')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['polarity_score']= data['text'].apply(lambda x : polarity(x))\n",
    "data['polarity_score'].hist().set(title='Distribution of Polarity Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Polarity of Reviews')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['polarity'] = data['polarity_score'].map(lambda x: sentiment(x))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(data.polarity.value_counts().index,\n",
    "        data.polarity.value_counts()) \n",
    "plt.title(\"Polarity of Reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(\"data/yelpSentiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def ner(text):\n",
    "    doc = nlp(text)\n",
    "    return [X.label_ for X in doc.ents]\n",
    "\n",
    "ent = data['text'].apply(lambda x : ner(x))\n",
    "ent = [x for sub in ent for x in sub]\n",
    "\n",
    "counter = Counter(ent)\n",
    "count = counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity frequencies visualization\n",
    "x,y = map(list,zip(*count))\n",
    "sns.barplot(x=y, y=x).set(title='Entity Classes in the Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then visualize token per entity\n",
    "\n",
    "def ner(text,ent=\"ORG\"):\n",
    "    doc = nlp(text)\n",
    "    return [X.text for X in doc.ents if X.label_ == ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a look at the date entities mentioned in the reviews\n",
    "org = data['text'].apply(lambda x: ner(x, \"DATE\"))\n",
    "org = [i for x in org for i in x]\n",
    "counter = Counter(org)\n",
    "\n",
    "x,y = map(list,zip(*counter.most_common(10)))\n",
    "sns.barplot(y, x).set(title='Most Mentioned Date Entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a look at the organizations mentioned in the reviews\n",
    "org = data['text'].apply(lambda x: ner(x, ent=\"ORG\"))\n",
    "org = [i for x in org for i in x]\n",
    "counter = Counter(org)\n",
    "\n",
    "x,y = map(list,zip(*counter.most_common(10)))\n",
    "sns.barplot(y, x).set(title='Most Mentioned Organizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a look at the places mentioned in the reviews\n",
    "org = data['text'].apply(lambda x: ner(x, \"PERSON\"))\n",
    "org = [i for x in org for i in x]\n",
    "counter = Counter(org)\n",
    "\n",
    "x,y = map(list,zip(*counter.most_common(10)))\n",
    "sns.barplot(y, x).set(title='Most Mentioned People')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(text):\n",
    "    pos=nltk.pos_tag(word_tokenize(text))\n",
    "    pos=list(map(list,zip(*pos)))[1]\n",
    "    return pos\n",
    "\n",
    "tags = data['text'].apply(lambda x : pos(x))\n",
    "tags=[x for l in tags for x in l]\n",
    "counter = Counter(tags)\n",
    "\n",
    "x,y = list(map(list,zip(*counter.most_common(7))))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Noun (NN)\n",
    "- Verb (VB)\n",
    "- Adjective (JJ)\n",
    "- Adverb(RB)\n",
    "- Preposition (IN)\n",
    "- Conjunction (CC)\n",
    "- Pronoun(PRP)\n",
    "- Interjection (INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drilling down even further: looking at NN\n",
    "def get_adjs(text):\n",
    "    adj=[]\n",
    "    pos = nltk.pos_tag(word_tokenize(text))\n",
    "    for word,tag in pos:\n",
    "        if tag=='NN':\n",
    "            adj.append(word)\n",
    "    return adj\n",
    "\n",
    "\n",
    "words = data['text'].apply(lambda x : get_adjs(x))\n",
    "words = [x for l in words for x in l]\n",
    "counter = Counter(words)\n",
    "\n",
    "x,y = list(map(list,zip(*counter.most_common(7))))\n",
    "sns.barplot(x=y, y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings with BERT (Hugging Face's transformer architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# be sure to download the pretrained transformer via the spaCy pipeline\n",
    "# !spacy download en_core_web_trf\n",
    "# !pip install spacy-transformers\n",
    "\n",
    "import en_core_web_trf\n",
    "\n",
    "txt = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "#Load the bert model\n",
    "nlp = en_core_web_trf.load()#spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "\n",
    "# get sentence embeddings from input text\n",
    "def get_embeddings(text):\n",
    "    return nlp(text).\n",
    "    \n",
    "    # Generating sentence embedding from the text\n",
    "# data['embeddings'] = data['text'].apply(get_embeddings)\n",
    "\n",
    "df = get_embeddings(txt)\n",
    "\n",
    "# it takes time to run BERT so its better to save the updated data for future use\n",
    "# data.to_csv(\"data/yelp_embeddings.csv\")\n",
    "\n",
    "# Transformer outputs sequences longer than the maximum length for the model (537 > 512). We will truncate the output.\n",
    "\n",
    "# df = pd.read_csv(\"data/yelp_embeddings.csv\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAug[\"polarity\"] = dataSentiment[\"polarity\"]\n",
    "dataAug.rename(columns={\"augmented\": \"text\"}, inplace = True)\n",
    "\n",
    "merged_df = dataSentiment.append(dataAug, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## Import libraries\n",
    "def clustering_question(data, NUM_CLUSTERS = 15):\n",
    "\n",
    "    sentences = data['text']\n",
    "\n",
    "    X = np.array(data['emb'].tolist())\n",
    "\n",
    "    kclusterer = KMeansClusterer(\n",
    "        NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,\n",
    "        repeats=25,avoid_empty_clusters=True)\n",
    "\n",
    "    assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "    data['cluster'] = pd.Series(assigned_clusters, index=data.index)\n",
    "    data['centroid'] = data['cluster'].apply(lambda x: kclusterer.means()[x])\n",
    "\n",
    "    return data, assigned_clusters\n",
    "\n",
    "def distance_from_centroid(row):\n",
    "    # type of emb and centroid is different, hence using tolist below\n",
    "    return distance_matrix([row['emb']], [row['centroid'].tolist()])[0][0]\n",
    "\n",
    "# Compute centroid distance to the data\n",
    "data['distance_from_centroid'] = data.apply(distance_from_centroid, axis=1)\n",
    "\n",
    "# df = dataSentiment.copy()\n",
    "df = data.copy()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Classifier Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding the classes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(dataSentiment['polarity'])\n",
    "\n",
    "dataSentiment['category'] = encoder.transform(dataSentiment['polarity'])\n",
    "merged_df['category'] = encoder.transform(merged_df['polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "def splitStep(df=dataSentiment, X=\"text\", y=\"category\", test_size=0.2, random_state=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(dataSentiment[X], dataSentiment[y], stratify=dataSentiment[y], test_size=test_size)\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "\n",
    "# word vectorizer\n",
    "def w2v(x, num_features=100): # xtrain & xtest will be accessed from the vectorizeStep (ignore warning)\n",
    "    import numpy as np\n",
    "    from gensim.models import Word2Vec\n",
    "\n",
    "    wordvec = Word2Vec(x, window=8, min_count=2, sample=1e-3, sg=1, workers=8) \n",
    "    vocab = set(wordvec.wv.index_to_key)\n",
    "    num_features = num_features\n",
    "\n",
    "    def average_word_vectors(tokens, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        ntokens = 0.\n",
    "        for t in tokens:\n",
    "            if t in vocabulary: \n",
    "                ntokens = ntokens + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[t])\n",
    "        if ntokens:\n",
    "            feature_vector = np.divide(feature_vector, ntokens)\n",
    "        return feature_vector\n",
    "\n",
    "    xTransformed = np.array([average_word_vectors(sent_tokens, wordvec, vocab, num_features) \n",
    "                    for sent_tokens in x]) \n",
    "    return xTransformed\n",
    "\n",
    "\n",
    "# document vectorizer\n",
    "def d2v(x):\n",
    "    from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "    docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(x)]\n",
    "    docvec = Doc2Vec(vector_size=100, window=3, min_count=4, workers=4, epochs=40)\n",
    "    docvec.build_vocab(docs)\n",
    "    docvec.train(docs, total_examples=docvec.corpus_count, epochs=docvec.epochs)\n",
    "\n",
    "    from gensim.utils import simple_preprocess\n",
    "    xTokenized = [simple_preprocess(h) for h in x]\n",
    "\n",
    "    xTransformed = [docvec.infer_vector(i) for i in xTokenized]\n",
    "\n",
    "    return xTransformed\n",
    "\n",
    "\n",
    "# transform data (options: TF-IDF, TF-IDF ngrams, word2vec, doc2vec)\n",
    "def vectorizeStep(inputData, fittingData=data.text, outputFormat=\"tfidf\"):\n",
    "    \n",
    "    # TF-IDF input\n",
    "    if outputFormat == \"tfidf\":\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer(\n",
    "                            strip_accents=\"unicode\", lowercase=True, analyzer='word', \n",
    "                            stop_words='english', max_df=0.95, min_df=0.05, max_features=500\n",
    "                                )\n",
    "        vectorizer.fit(fittingData)\n",
    "        \n",
    "        transformedData = vectorizer.transform(inputData)\n",
    "    \n",
    "    # TF-IDF ngrams input\n",
    "    elif outputFormat == \"ngrams\":\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer(\n",
    "                            strip_accents=\"unicode\", lowercase=True, analyzer='word', ngram_range=(2,3), \n",
    "                            max_df=0.95, min_df=0.05, max_features=500\n",
    "                            )\n",
    "        vectorizer.fit(fittingData)\n",
    "        \n",
    "        transformedData = vectorizer.transform(inputData)\n",
    "\n",
    "    # word vector input\n",
    "    elif outputFormat == \"word2vec\":\n",
    "        transformedData = w2v(inputData)\n",
    "\n",
    "    # document vector input\n",
    "    elif outputFormat == \"doc2vec\":\n",
    "        transformedData = d2v(inputData)\n",
    "        \n",
    "    return transformedData\n",
    "\n",
    "\n",
    "# train models\n",
    "def trainStep(classifier: str, x_input, y_input):\n",
    "    \n",
    "    # import GridSearchCV\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # create a SVC model using GridSearchCV\n",
    "    if classifier == \"svc\":\n",
    "        \n",
    "        # import SVC\n",
    "        from sklearn.svm import SVC\n",
    "\n",
    "        param_grid = {\n",
    "                        'C': [0.1, 1, 10, 100, 1000],'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                        'kernel': ['linear', 'rbf', 'sigmoid']} \n",
    "        \n",
    "\n",
    "        \n",
    "        model = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "\n",
    "    # create an XGBoost model\n",
    "    elif classifier == \"xgboost\":\n",
    "        import xgboost\n",
    "        model = xgboost.XGBClassifier()\n",
    "\n",
    "    # create a knn model\n",
    "    elif classifier == \"knn\":\n",
    "\n",
    "        # import KNN\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "        k_range = list(range(1, 31))\n",
    "        param_grid = dict(n_neighbors=k_range)\n",
    "        \n",
    "        model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, scoring='accuracy')\n",
    "    \n",
    "    m = model.fit(x_input, y_input)\n",
    "    \n",
    "    return m\n",
    "\n",
    "\n",
    "# predict on the xtest \n",
    "def predictStep(trainedModel, x_input):\n",
    "    predictions = trainedModel.predict(x_input)\n",
    "    return predictions\n",
    "\n",
    "# evaluate on ytest \n",
    "def evaluateStep(ytrue, ypred):\n",
    "    from sklearn.metrics import precision_recall_fscore_support as score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    precision,recall,fscore,_=score(ytrue,ypred, average='macro')\n",
    "    accuracy = accuracy_score(ytrue,ypred)\n",
    "\n",
    "    return round(precision,4), round(recall,4), round(fscore,4), round(accuracy,4) #,roc_auc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store f1 scores of different models\n",
    "f1ScoreMetrics = {'xgboost': [],'knn': []}\n",
    "\n",
    "# dictionary to store accuracy scores of different models\n",
    "accuracyScoreMetrics = {'xgboost': [],'knn': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\echemochek\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:55:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\echemochek\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:55:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\echemochek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# train-test split the data \n",
    "xTrn, xTst, yTrn, yTst = splitStep(df=data, X=\"text\", y=\"category\")\n",
    "\n",
    "# models and inputs\n",
    "modelList = ['xgboost', 'knn']\n",
    "dataFormats = ['tfidf', 'ngrams']\n",
    "trainedModels = [] # to store the models we train to use later\n",
    "\n",
    "# fit model, train on training data, test on the testing set, append scores to dictionaries\n",
    "for model in modelList:\n",
    "    for dataFormat in dataFormats:\n",
    "\n",
    "        # vectorize\n",
    "        xtrainVectorized = vectorizeStep(xTrn, outputFormat=dataFormat)\n",
    "        xtestVectorized = vectorizeStep(xTst, outputFormat=dataFormat)\n",
    "\n",
    "        # train\n",
    "        trainedModel = trainStep(model, xtrainVectorized, yTrn)\n",
    "\n",
    "        # predict\n",
    "        ypreds = predictStep(trainedModel, xtestVectorized)\n",
    "\n",
    "        # evaluate\n",
    "        metrics = evaluateStep(yTst, ypreds)\n",
    "\n",
    "        # append trained models to a list and metrics to a dictionary\n",
    "        trainedModels.append(trainedModel) \n",
    "        f1ScoreMetrics[model].append(metrics[2])\n",
    "        accuracyScoreMetrics[model].append(metrics[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for highlighting best model\n",
    "def highlight_cells(val):\n",
    "    color = 'yellow' if val == maxVal else ''\n",
    "    return 'background-color: {}'.format(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6e447_row0_col0 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_6e447_row0_col1, #T_6e447_row1_col0, #T_6e447_row1_col1 {\n",
       "  background-color: ;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6e447_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >xgboost</th>\n",
       "      <th class=\"col_heading level0 col1\" >knn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6e447_level0_row0\" class=\"row_heading level0 row0\" >tfidf</th>\n",
       "      <td id=\"T_6e447_row0_col0\" class=\"data row0 col0\" >0.524900</td>\n",
       "      <td id=\"T_6e447_row0_col1\" class=\"data row0 col1\" >0.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e447_level0_row1\" class=\"row_heading level0 row1\" >ngrams</th>\n",
       "      <td id=\"T_6e447_row1_col0\" class=\"data row1 col0\" >0.396800</td>\n",
       "      <td id=\"T_6e447_row1_col1\" class=\"data row1 col1\" >0.361400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ee9aef7d00>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display f1 scores in a formatted dataframe\n",
    "f1_df = pd.DataFrame.from_dict(f1ScoreMetrics)\n",
    "f1_df.set_index([pd.Index(dataFormats)], inplace=True)\n",
    "\n",
    "maxVal = f1_df.max().max()\n",
    "f1_df.style.applymap(highlight_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_13947_row0_col0 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_13947_row0_col1, #T_13947_row1_col0, #T_13947_row1_col1 {\n",
       "  background-color: ;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_13947_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >xgboost</th>\n",
       "      <th class=\"col_heading level0 col1\" >knn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_13947_level0_row0\" class=\"row_heading level0 row0\" >tfidf</th>\n",
       "      <td id=\"T_13947_row0_col0\" class=\"data row0 col0\" >0.744500</td>\n",
       "      <td id=\"T_13947_row0_col1\" class=\"data row0 col1\" >0.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_13947_level0_row1\" class=\"row_heading level0 row1\" >ngrams</th>\n",
       "      <td id=\"T_13947_row1_col0\" class=\"data row1 col0\" >0.622000</td>\n",
       "      <td id=\"T_13947_row1_col1\" class=\"data row1 col1\" >0.591000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ee9b45f8b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display recall scores in a formatted dataframe\n",
    "accuracy_df = pd.DataFrame.from_dict(accuracyScoreMetrics)\n",
    "accuracy_df.set_index([pd.Index(dataFormats)], inplace=True)\n",
    "\n",
    "maxVal = accuracy_df.max().max()\n",
    "accuracy_df.style.applymap(highlight_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the winning model is stored in the trainedModels list in index 4\n",
    "winningModel = trainedModels[0]\n",
    "winningModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use the model on augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albumentation = pd.read_csv(\"data/yelp_augmented_syn_ant_5K_only.csv\")\n",
    "df_albumentation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us create on large dataset! \n",
    "df_full = df_albumentation['text']\\\n",
    "            .append(df_albumentation['augmented'])\\\n",
    "                .append(df_albumentation['syn_augmented'])\\\n",
    "                    .append(df_albumentation['ant_augmented'])\\\n",
    "                        .reset_index(drop=True)\n",
    "\n",
    "# create a new dataframe with the same name from the data\n",
    "df_full = pd.DataFrame(data=df_full, columns=['text'], dtype=str) # this new dataframe has 20000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "creating labels (sentiment for this dataset using the defined method in earlier steps\n",
    "we do this instead of using labels from the parent column because the albumentation may \n",
    "have changed sentiments for some observations\n",
    "'''\n",
    "df_full['polarity_score']= df_full['text'].apply(lambda x : polarity(str(x)))\n",
    "df_full['polarity'] = df_full['polarity_score'].map(lambda x: sentiment(x))\n",
    "\n",
    "# we drop any potential duplicates - albumentation is not perfect\n",
    "df_full.drop_duplicates(inplace=True)\n",
    "\n",
    "# save dataframe with specific columns as new dataset so we can easily load it next time\n",
    "# df_full[['text', 'polarity']].to_csv(\"data/yelpAugmentedFinal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data with final data\n",
    "df = pd.read_csv(\"data/yelpAugmentedFinal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store f1 scores of different models\n",
    "f1ScoreMetricsAug = {'xgboost': []}\n",
    "\n",
    "# dictionary to store accuracy scores of different models\n",
    "accuracyScoreMetricsAug = {'xgboost': []}\n",
    "\n",
    "# drop any null values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# save as training\n",
    "xTrnAug = df[\"text\"]\n",
    "yTrnAug = df[\"polarity\"]\n",
    "\n",
    "\n",
    "# vectorize\n",
    "xtrainVectorizedAug = vectorizeStep(xTrnAug, fittingData=df.text)\n",
    "\n",
    "#xtestVectorized = vectorizeStep(xTst, outputFormat=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\echemochek\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:38:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\echemochek\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:39:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\echemochek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# drop null values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# label encode and add new column\n",
    "df['category'] = encoder.transform(df['polarity'])\n",
    "\n",
    "# save as training\n",
    "xTrnAug = df[\"text\"]\n",
    "yTrnAug = df['category']\n",
    "\n",
    "\n",
    "# models and inputs\n",
    "modelList = ['xgboost', 'knn']\n",
    "dataFormats = ['tfidf', 'ngrams']\n",
    "trainedModelsAug = [] # to store the models we train to use later\n",
    "\n",
    "# fit model, train on training data, test on the testing set, append scores to dictionaries\n",
    "for model in modelList:\n",
    "    for dataFormat in dataFormats:\n",
    "\n",
    "        # vectorize\n",
    "        xtrainVectorized = vectorizeStep(xTrnAug, outputFormat=dataFormat)\n",
    "        xtestVectorized = vectorizeStep(xTst, outputFormat=dataFormat)\n",
    "\n",
    "        # train\n",
    "        trainedModel = trainStep(model, xtrainVectorized, yTrnAug)\n",
    "\n",
    "        # predict\n",
    "        ypreds = predictStep(trainedModel, xtestVectorized)\n",
    "\n",
    "        # evaluate\n",
    "        metrics = evaluateStep(yTst, ypreds)\n",
    "\n",
    "        # append trained models to a list and metrics to a dictionary\n",
    "        trainedModelsAug.append(trainedModel) \n",
    "        f1ScoreMetricsAug[model].append(metrics[2])\n",
    "        accuracyScoreMetricsAug[model].append(metrics[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e8c0c_row0_col0 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_e8c0c_row0_col1, #T_e8c0c_row1_col0, #T_e8c0c_row1_col1 {\n",
       "  background-color: ;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e8c0c_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >xgboost</th>\n",
       "      <th class=\"col_heading level0 col1\" >knn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e8c0c_level0_row0\" class=\"row_heading level0 row0\" >tfidf</th>\n",
       "      <td id=\"T_e8c0c_row0_col0\" class=\"data row0 col0\" >0.738900</td>\n",
       "      <td id=\"T_e8c0c_row0_col1\" class=\"data row0 col1\" >0.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8c0c_level0_row1\" class=\"row_heading level0 row1\" >ngrams</th>\n",
       "      <td id=\"T_e8c0c_row1_col0\" class=\"data row1 col0\" >0.479400</td>\n",
       "      <td id=\"T_e8c0c_row1_col1\" class=\"data row1 col1\" >0.573700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ee99865280>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display f1 scores in a formatted dataframe\n",
    "f1_df = pd.DataFrame.from_dict(f1ScoreMetrics)\n",
    "f1_df.set_index([pd.Index(dataFormats)], inplace=True)\n",
    "\n",
    "maxVal = f1_df.max().max()\n",
    "f1_df.style.applymap(highlight_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0f677_row0_col0 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_0f677_row0_col1, #T_0f677_row1_col0, #T_0f677_row1_col1 {\n",
       "  background-color: ;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0f677_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >xgboost</th>\n",
       "      <th class=\"col_heading level0 col1\" >knn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0f677_level0_row0\" class=\"row_heading level0 row0\" >tfidf</th>\n",
       "      <td id=\"T_0f677_row0_col0\" class=\"data row0 col0\" >0.848500</td>\n",
       "      <td id=\"T_0f677_row0_col1\" class=\"data row0 col1\" >0.760500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f677_level0_row1\" class=\"row_heading level0 row1\" >ngrams</th>\n",
       "      <td id=\"T_0f677_row1_col0\" class=\"data row1 col0\" >0.717500</td>\n",
       "      <td id=\"T_0f677_row1_col1\" class=\"data row1 col1\" >0.733000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ee9b46a2b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display recall scores in a formatted dataframe\n",
    "accuracy_df = pd.DataFrame.from_dict(accuracyScoreMetrics)\n",
    "accuracy_df.set_index([pd.Index(dataFormats)], inplace=True)\n",
    "\n",
    "maxVal = accuracy_df.max().max()\n",
    "accuracy_df.style.applymap(highlight_cells)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a34fec39bca19239869142c7bb5030492fb37ddeaee88d8e8fdb42aa6668f7ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
